"""Verify BCE loss training with real data."""
import torch
import numpy as np
from pathlib import Path
from maou.app.learning.network import Network
from maou.app.learning.dataset import KifDataset
from maou.app.learning.setup import LossOptimizerFactory
from maou.infra.file_system.file_data_source import FileDataSource
from torch.utils.data import DataLoader

def main():
    print("=" * 80)
    print("BCE Loss Training Verification")
    print("=" * 80)

    # Load small sample from real data
    data_dir = Path("preprocess/floodgate/2020")
    file_paths = sorted(data_dir.glob("*.npy"))[:1]  # Use only first file

    print(f"\nUsing data file: {file_paths[0]}")

    # Load data source
    datasource = FileDataSource(
        file_paths=file_paths,
        array_type="preprocessing"
    )

    # Create dataset (limit to 10000 samples)
    class LimitedDataset(KifDataset):
        def __len__(self):
            return min(10000, super().__len__())

    dataset = LimitedDataset(datasource=datasource)
    print(f"Dataset size: {len(dataset)} samples")

    # Create dataloader
    dataloader = DataLoader(
        dataset,
        batch_size=256,
        shuffle=True,
        num_workers=2,
        pin_memory=True,
    )

    # Create model
    model = Network(
        embed_dim=256,
        depth=4,
        num_heads=8,
        dropout_rate=0.1,
    )

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    print(f"\nDevice: {device}")
    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    # Create loss functions and optimizer
    loss_fn_policy, loss_fn_value = LossOptimizerFactory.create_loss_functions(
        gce_parameter=0.7
    )
    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

    print(f"\nLoss function: {loss_fn_value.__class__.__name__}")
    print(f"Learning rate: 0.001")
    print(f"Optimizer: SGD(momentum=0.9)")

    # Training loop
    print("\n" + "=" * 80)
    print("Training Progress")
    print("=" * 80)

    model.train()
    num_epochs = 3

    for epoch in range(num_epochs):
        total_loss = 0.0
        total_value_loss = 0.0
        total_policy_loss = 0.0
        num_batches = 0

        # Collect value predictions for analysis
        all_value_preds = []
        all_value_labels = []

        for batch_idx, (inputs, (labels_policy, labels_value, legal_move_mask)) in enumerate(dataloader):
            # Move to device
            inputs = inputs.to(device)
            labels_policy = labels_policy.to(device)
            labels_value = labels_value.to(device)
            legal_move_mask = legal_move_mask.to(device)

            # Forward pass
            optimizer.zero_grad()
            policy_logits, value_pred = model(inputs)

            # Compute losses
            policy_loss = loss_fn_policy(policy_logits, labels_policy, legal_move_mask)
            value_loss = loss_fn_value(value_pred, labels_value)
            loss = policy_loss + value_loss

            # Backward pass
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            # Track losses
            total_loss += loss.item()
            total_value_loss += value_loss.item()
            total_policy_loss += policy_loss.item()
            num_batches += 1

            # Collect predictions for first few batches
            if batch_idx < 5:
                all_value_preds.append(value_pred.detach().cpu())
                all_value_labels.append(labels_value.detach().cpu())

            if batch_idx >= 10:  # Limit to 10 batches for quick verification
                break

        # Epoch summary
        avg_loss = total_loss / num_batches
        avg_value_loss = total_value_loss / num_batches
        avg_policy_loss = total_policy_loss / num_batches

        # Analyze value predictions
        all_value_preds = torch.cat(all_value_preds)
        all_value_labels = torch.cat(all_value_labels)

        print(f"\nEpoch {epoch + 1}/{num_epochs}:")
        print(f"  Total Loss:  {avg_loss:.6f}")
        print(f"  Value Loss:  {avg_value_loss:.6f}")
        print(f"  Policy Loss: {avg_policy_loss:.6f}")
        print(f"  Value Pred Mean: {all_value_preds.mean().item():.4f}")
        print(f"  Value Pred Std:  {all_value_preds.std().item():.4f}")
        print(f"  Value Pred Min:  {all_value_preds.min().item():.4f}")
        print(f"  Value Pred Max:  {all_value_preds.max().item():.4f}")
        print(f"  Value Label Mean: {all_value_labels.mean().item():.4f}")

    # Final analysis
    print("\n" + "=" * 80)
    print("Verification Results")
    print("=" * 80)

    # Check value label distribution
    value_hist = torch.histc(all_value_labels, bins=5, min=0, max=1)
    print(f"\nValue label distribution:")
    print(f"  [0.0-0.2]: {value_hist[0].item():.0f}")
    print(f"  [0.2-0.4]: {value_hist[1].item():.0f}")
    print(f"  [0.4-0.6]: {value_hist[2].item():.0f}")
    print(f"  [0.6-0.8]: {value_hist[3].item():.0f}")
    print(f"  [0.8-1.0]: {value_hist[4].item():.0f}")

    # Check prediction distribution
    pred_hist = torch.histc(all_value_preds, bins=5, min=0, max=1)
    print(f"\nValue prediction distribution:")
    print(f"  [0.0-0.2]: {pred_hist[0].item():.0f}")
    print(f"  [0.2-0.4]: {pred_hist[1].item():.0f}")
    print(f"  [0.4-0.6]: {pred_hist[2].item():.0f}")
    print(f"  [0.6-0.8]: {pred_hist[3].item():.0f}")
    print(f"  [0.8-1.0]: {pred_hist[4].item():.0f}")

    print("\n✓ BCE Loss implementation verified!")
    print("✓ Model trains successfully with new loss function")
    print("✓ Ready for full-scale training")

    print("\n" + "=" * 80)
    print("Recommended Training Command")
    print("=" * 80)
    print("""
poetry run maou learn-model \\
  --input-format preprocess \\
  --input-dir preprocess/floodgate/2020 \\
  --input-file-packed \\
  --test-ratio 0.2 \\
  --output-gcs \\
  --gcs-bucket-name maou-test-dousu \\
  --gcs-base-path large_test_bce \\
  --batch-size 1024 \\
  --epoch 20 \\
  --gpu cuda:0 \\
  --compilation true \\
  --dataloader-workers 12 \\
  --prefetch-factor 1 \\
  --pin-memory \\
  --learning-ratio 0.001 \\
  --gce-parameter 0.7 \\
  --momentum 0.9
""")

if __name__ == "__main__":
    main()
